{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "73124cac-67c3-458a-a643-78dd50067938",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=spark-sql, master=local) created by getOrCreate at /tmp/ipykernel_3777/4175860230.py:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf, SparkContext\n\u001b[1;32m      2\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark_sql_basic2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m sc   \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=spark-sql, master=local) created by getOrCreate at /tmp/ipykernel_3777/4175860230.py:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"spark_sql_basic2\")\n",
    "sc   = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c755ee-9483-45d9-8790-cb8507c97f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD만을 이용한 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62064931-39fb-4d70-bb94-ecc6300b3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "movies_rdd = sc.parallelize([\n",
    "    (1, (\"어벤져스\", \"마블\")),\n",
    "    (2, (\"슈퍼맨\", \"DC\")),\n",
    "    (3, (\"배트맨\", \"DC\")),\n",
    "    (4, (\"겨울왕국\", \"디즈니\")),\n",
    "    (5, (\"아이언맨\", \"마블\"))\n",
    "])\n",
    "\n",
    "\n",
    "attendances_rdd = sc.parallelize([\n",
    "    (1, (13934592, \"KR\")),\n",
    "    (2, (2182227,\"KR\")),\n",
    "    (3, (4226242, \"KR\")),\n",
    "    (4, (10303058, \"KR\")),\n",
    "    (5, (4300365, \"KR\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee708262-d2a1-428a-90b4-5420d8cac134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마블 영화 중 관객 수가 500만 이상인 영화를 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ffe117c-6f85-47ea-8438-0b5b4b827d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (('슈퍼맨', 'DC'), (2182227, 'KR'))),\n",
       " (4, (('겨울왕국', '디즈니'), (10303058, 'KR'))),\n",
       " (1, (('어벤져스', '마블'), (13934592, 'KR'))),\n",
       " (3, (('배트맨', 'DC'), (4226242, 'KR'))),\n",
       " (5, (('아이언맨', '마블'), (4300365, 'KR')))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CASE1. join 먼저, filter 나중에\n",
    "movie_att = movies_rdd.join(attendances_rdd)\n",
    "movie_att.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46572ff5-443f-4eb7-9bd8-9d2f29f28b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "movie_att.filter(\n",
    "    lambda x : x[1][0][1] == \"마블\" and x[1][1][0] > 5000000\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b1043-e899-4a72-8a63-0c4e70ff428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE 2. filter 먼저, join 나중에\n",
    "filtered_movies = movies_rdd.filter(lambda x : x[1][1] == '마블')\n",
    "filtered_att = attendances_rdd.filter(lambda x : x[1][0] > 5000000)\n",
    "\n",
    "filtered_movies.join(filtered_att).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf052595-fa92-47a4-adb9-6a1cc5ea2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f19c7-bd4a-417e-902d-26b7aa3fbfec",
   "metadata": {},
   "source": [
    "# Spark SQL 사용해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1d6a6be2-6364-42ad-972f-ee733e41b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"spark-sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6d704416-8c08-42dd-abcb-c7c10b3de900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 추가\n",
    "movies = [\n",
    "    (1, \"어벤져스\", \"마블\", 2012, 4, 26),\n",
    "    (2, \"슈퍼맨\", \"DC\", 2013, 6, 13),\n",
    "    (3, \"배트맨\", \"DC\", 2008, 8, 6),\n",
    "    (4, \"겨울왕국\", \"디즈니\", 2014, 1, 16),\n",
    "    (5, \"아이언맨\", \"마블\", 2008, 4, 30)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "445a1c74-a982-41b3-b420-81391fa8035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#스키마를 알아야 한다.\n",
    "movie_schema = [\"id\", \"name\", \"company\", \"year\", \"month\", \"day\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8b433-d021-4fb2-bdd2-5ec2e89934e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터 프레임 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f9d4f62e-fac5-42f7-9c42-6ce6edaf3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=movies, schema=movie_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e74ccd-1812-4a2e-90b3-025b7e6a74c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('name', 'string'),\n",
       " ('company', 'string'),\n",
       " ('year', 'bigint'),\n",
       " ('month', 'bigint'),\n",
       " ('day', 'bigint')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b154ba30-85d0-4cad-a4cc-199924c3343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+----+-----+---+\n",
      "| id|    name|company|year|month|day|\n",
      "+---+--------+-------+----+-----+---+\n",
      "|  1|어벤져스|   마블|2012|    4| 26|\n",
      "|  2|  슈퍼맨|     DC|2013|    6| 13|\n",
      "|  3|  배트맨|     DC|2008|    8|  6|\n",
      "|  4|겨울왕국| 디즈니|2014|    1| 16|\n",
      "|  5|아이언맨|   마블|2008|    4| 30|\n",
      "+---+--------+-------+----+-----+---+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80287e18-75e6-4831-b880-fe91ca53a3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|company|\n",
      "+-------+\n",
      "|   마블|\n",
      "|     DC|\n",
      "|     DC|\n",
      "| 디즈니|\n",
      "|   마블|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"company\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff4f4969-80e2-4926-a2ca-63d09f45a5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|어벤져스|\n",
      "|  슈퍼맨|\n",
      "|  배트맨|\n",
      "|겨울왕국|\n",
      "|아이언맨|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20312673-ba68-45f6-8c39-d79f6687285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+----+-----+---+\n",
      "| id|    name|company|year|month|day|\n",
      "+---+--------+-------+----+-----+---+\n",
      "|  1|어벤져스|   마블|2012|    4| 26|\n",
      "|  4|겨울왕국| 디즈니|2014|    1| 16|\n",
      "|  5|아이언맨|   마블|2008|    4| 30|\n",
      "+---+--------+-------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['day'] >= 15).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37e30558-0987-4151-a402-03d18985dfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+----+-----+---+\n",
      "| id|    name|company|year|month|day|\n",
      "+---+--------+-------+----+-----+---+\n",
      "|  1|어벤져스|   마블|2012|    4| 26|\n",
      "|  2|  슈퍼맨|     DC|2013|    6| 13|\n",
      "|  3|  배트맨|     DC|2008|    8|  6|\n",
      "|  4|겨울왕국| 디즈니|2014|    1| 16|\n",
      "|  5|아이언맨|   마블|2008|    4| 30|\n",
      "+---+--------+-------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.year >= 2000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74969196-f535-45f1-8fa1-e5897917dd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+----+-----+---+\n",
      "| id|    name|company|year|month|day|\n",
      "+---+--------+-------+----+-----+---+\n",
      "|  2|  슈퍼맨|     DC|2013|    6| 13|\n",
      "|  4|겨울왕국| 디즈니|2014|    1| 16|\n",
      "+---+--------+-------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2013년 이후 영화만 꺼내기\n",
    "df.filter(df.year >= 2013).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d939f89-5675-4a90-b4b6-1a62de4a009d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+----+-----+---+\n",
      "| id|    name|company|year|month|day|\n",
      "+---+--------+-------+----+-----+---+\n",
      "|  1|어벤져스|   마블|2012|    4| 26|\n",
      "|  5|아이언맨|   마블|2008|    4| 30|\n",
      "+---+--------+-------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 마블영화만 꺼내기 \n",
    "df.filter(df.company == '마블').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e9daf939-9807-4f83-a082-8bc47b50aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df를 \"movies\"라는 이름의 임시 SQL 뷰로 등록함\n",
    "\n",
    "df.createOrReplaceTempView(\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bd2369ec-d17d-4c07-87e2-75f236509f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|어벤져스|\n",
      "|  슈퍼맨|\n",
      "|  배트맨|\n",
      "|겨울왕국|\n",
      "|아이언맨|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 영화 이름만 가져오기\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "SELECT name\n",
    "  FROM movies\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aabc1ad-e8c4-4a7e-8c3e-717026067296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+----+-----+---+\n",
      "| id|    name|company|year|month|day|\n",
      "+---+--------+-------+----+-----+---+\n",
      "|  1|어벤져스|   마블|2012|    4| 26|\n",
      "|  2|  슈퍼맨|     DC|2013|    6| 13|\n",
      "|  3|  배트맨|     DC|2008|    8|  6|\n",
      "|  5|아이언맨|   마블|2008|    4| 30|\n",
      "+---+--------+-------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#영화 이름만 가져오기 \n",
    "df.filter(df.company.isin('마블','DC')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f24eb187-722d-495b-af67-a375318d5ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+----+-----+---+\n",
      "| id|    name|company|year|month|day|\n",
      "+---+--------+-------+----+-----+---+\n",
      "|  1|어벤져스|   마블|2012|    4| 26|\n",
      "|  2|  슈퍼맨|     DC|2013|    6| 13|\n",
      "|  4|겨울왕국| 디즈니|2014|    1| 16|\n",
      "+---+--------+-------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2010년 이후에 개봉한 영화를 조회\n",
    "\n",
    "query = \"\"\"\n",
    "select *\n",
    "from movies\n",
    "where year >= 2010\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a6585-3b8e-43ef-9686-3e545c56a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012년도 이전에 개봉한 영화의 이름과 회사를 출력\n",
    "query = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a37e2-975c-4ffb-afd8-93f3fc7d9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# like 문자열 데이터에서 특정 단어나 문장을 포함한 데이터를 찾을 때\n",
    "# % 기호를 사용해서 문장이 매칭되는지 확인 가능!\n",
    "# 제목이 ~~맨으로 끝나는 데이터의 모든 정보를 조회\n",
    "query = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1e78e-3314-434c-ae29-0d698ceb5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BETWEEN 특정 데이터와 데이터 사이를 조회\n",
    "\n",
    "# 개봉 월이 4 ~ 8월 사이. 4 <= 개봉월 <= 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0a753-fac1-4151-a016-c09511132f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20690144-20a2-4126-888c-b1074bd835bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc365151-3703-444c-a100-48ad4046e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attendances = [\n",
    "    (1, 13934592., \"KR\"),\n",
    "    (2, 2182227.,\"KR\"),\n",
    "    (3, 4226242., \"KR\"),\n",
    "    (4, 10303058., \"KR\"),\n",
    "    (5, 4300365., \"KR\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395f667-08ab-4e02-bbf8-dad62b1b9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 스키마 지정해 보기\n",
    "from pyspark.sql.types import StringType, FloatType\\\n",
    "    , IntegerType\\\n",
    "    , StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d3d10-b99b-4677-b203-459d4e73b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_schema = StructType([ # 모든 컬럼의 타입을 통칭 - 컬럼 데이터의 집합\n",
    "    StructField(\"id\", IntegerType(), True), # StructField : 컬럼\n",
    "    StructField(\"att\", FloatType(), True),\n",
    "    StructField(\"theater_country\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49e812-3a29-4b4b-8de1-3ad36b1da102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "att_df = spark.createDataFrame(\n",
    "    data=attendances,\n",
    "    schema=att_schema\n",
    ")\n",
    "\n",
    "att_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1c138-86c5-4ddd-812e-301655451a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_df.createOrReplaceTempView(\"att\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa55fe8-7c9a-4c94-9d84-a413126af7a3",
   "metadata": {},
   "source": [
    "# join 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7167b328-0c32-4159-870b-e91a9e8c36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attendances = [\n",
    "    (1, 13934592., \"KR\"),\n",
    "    (2, 2182227.,\"KR\"),\n",
    "    (3, 4226242., \"KR\"),\n",
    "    (4, 10303058., \"KR\"),\n",
    "    (5, 4300365., \"KR\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0e28ade1-0b13-4494-a968-b87ee30df0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 직접 스키마 지정해 보기\n",
    "from pyspark.sql.types import StringType, FloatType\\\n",
    "    , IntegerType\\\n",
    "    , StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "138de7fb-2c5a-432f-9825-922e0537a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "att_schema = StructType([ # 모든 컬럼의 타입을 통칭 - 컬럼 데이터의 집합\n",
    "    StructField(\"id\", IntegerType(), True), # StructField : 컬럼\n",
    "    StructField(\"att\", FloatType(), True),\n",
    "    StructField(\"theater_country\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "47e94e29-6317-4d50-b872-e70614f61e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'), ('att', 'float'), ('theater_country', 'string')]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "att_df = spark.createDataFrame(\n",
    "    data=attendances,\n",
    "    schema=att_schema\n",
    ")\n",
    "\n",
    "att_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c9267fd0-e015-41b6-8c91-9e99fdcff3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "att_df.createOrReplaceTempView(\"att\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "26768eb9-a848-4518-912c-381a48a1bb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------------+\n",
      "| id|        att|theater_country|\n",
      "+---+-----------+---------------+\n",
      "|  1|1.3934592E7|             KR|\n",
      "|  2|  2182227.0|             KR|\n",
      "|  3|  4226242.0|             KR|\n",
      "|  4|1.0303058E7|             KR|\n",
      "|  5|  4300365.0|             KR|\n",
      "+---+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "att_df.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "73edfafa-b377-4c9b-9173-f7568a42d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+-----------+\n",
      "| id|    name|company|        att|\n",
      "+---+--------+-------+-----------+\n",
      "|  1|어벤져스|   마블|1.3934592E7|\n",
      "|  2|  슈퍼맨|     DC|  2182227.0|\n",
      "|  3|  배트맨|     DC|  4226242.0|\n",
      "|  4|겨울왕국| 디즈니|1.0303058E7|\n",
      "|  5|아이언맨|   마블|  4300365.0|\n",
      "+---+--------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df와 join\n",
    "\n",
    "query = '''\n",
    "select movies.id, movies.name , movies.company, att.att\n",
    "from movies\n",
    "join att ON movies.id = att.id\n",
    "'''\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950f135-dac2-4c75-a344-324c57f7d9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f761ec8-7ac9-4e26-b274-cf7b4f669c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13061d-de94-4f56-9821-a75dacd13e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea8b49-beed-46a0-ab5e-220852e30aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5e568-23bb-47a0-9c9a-d8ee8581c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select\n",
    "df.select(\"*\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73334616-316a-47ba-bbca-3780b58fe6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\", \"company\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db9364-606b-4974-873a-5fd5ef3a8918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.name, (df.year-2000).alias(\"year\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb58352-d6b8-4545-a335-632d52473def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg : Aggreagte의 약자로써, 그룹핑 후 데이터를 하나로 합쳐주는 역할\n",
    "df.agg({\"id\": \"count\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c9f7c-6836-4924-8f08-b01e5272d541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51212c6f-0199-49ba-9995-09783594c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df.agg(F.min(df.year)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec3f7a-c11a-4aa8-b50c-9abfd0eb806c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d60a1-222c-4e5a-8a73-ec74fc187862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy().avg().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03955127-cabe-4020-803a-d80605ec090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회사별 개봉월의 평균\n",
    "df.groupBy('company').agg({\"month\": \"mean\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94641161-4fda-4170-97a6-0f04728b4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회사 별 월 별 영화 개수 정보\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8807e25-22f4-439a-91fe-e51c5ce34f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join : 다른 데이터 프레임과 사용자가 지정한 컬럼을 기준으로 합치는 작업\n",
    "df.join(att_df, 'id').select(df.name, att_df.att).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973769b0-f1cd-4b87-8172-805f3bcdce85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15abf176-7d4b-4233-bba1-bd032c66ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select, where, orderBy 절 사용\n",
    "marvel_df = df.select(\"name\", \"company\", \"year\").where(\"company=='마블'\").orderBy(\"id\")\n",
    "marvel_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "195c87a5-3db2-41d8-a6da-b47d6e030e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327cca2-6eca-4090-b1e1-004f28e598ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c205c-c9d3-4f7f-90bf-aeeffda08ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6824043e-9457-4f3f-9ca5-f3aa89b29087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"trip_count_sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "00b00cb7-73f7-4011-8b53-b0df55938fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_file = \"learning_spark_data/fhvhv_tripdata_2020-03.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "071ddd29-2ab5-412b-be45-0d067175d321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inferSchema : 자동으로 스키마 예측하게 하기\n",
    "data = spark.read.csv(trip_file, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837bd54d-1d5c-4cbb-bc09-76ecbe678e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9fdfa452-8e10-4401-802c-b7fb1df93aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"mobility_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9d4843ad-cdc8-40de-9970-9e8ba15b32bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2020-03-01 00:03:40|2020-03-01 00:23:39|          81|         159|   NULL|\n",
      "|           HV0005|              B02510|2020-03-01 00:28:05|2020-03-01 00:38:57|         168|         119|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:03:07|2020-03-01 00:15:04|         137|         209|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:18:42|2020-03-01 00:38:42|         209|          80|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:44:24|2020-03-01 00:58:44|         256|         226|   NULL|\n",
      "|           HV0003|              B02682|2020-03-01 00:17:23|2020-03-01 00:39:35|          79|         263|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:01:18|2020-03-01 00:38:52|          61|          29|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:43:27|2020-03-01 00:47:27|         150|         150|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:52:23|2020-03-01 01:00:15|         150|         210|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:19:49|2020-03-01 00:23:40|          60|         167|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:29:34|2020-03-01 00:39:19|          47|         213|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:41:44|2020-03-01 00:58:13|         213|         235|   NULL|\n",
      "|           HV0003|              B02765|2020-03-01 00:11:26|2020-03-01 00:24:46|         243|         153|   NULL|\n",
      "|           HV0003|              B02765|2020-03-01 00:28:05|2020-03-01 00:38:56|         127|          18|   NULL|\n",
      "|           HV0003|              B02765|2020-03-01 00:44:28|2020-03-01 00:52:09|          18|         169|   NULL|\n",
      "|           HV0003|              B02765|2020-03-01 00:56:50|2020-03-01 00:59:26|          94|         169|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:56:14|2020-03-01 01:03:38|         211|         158|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:14:15|2020-03-01 00:26:47|         246|         107|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:31:38|2020-03-01 00:58:07|         234|           9|   NULL|\n",
      "|           HV0005|              B02510|2020-03-01 00:26:31|2020-03-01 00:38:07|         139|          10|      1|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select *\n",
    "from mobility_data\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f9776-6194-4eeb-9144-96f88802f396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0071187-c4f9-44f5-8716-ab5cbef1fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 SQL을 사용하는 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6b8134e5-bb59-411d-9d1c-c31bb358beee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|pickup_date| trips|\n",
      "+-----------+------+\n",
      "| 2020-03-03|697880|\n",
      "| 2020-03-02|648986|\n",
      "| 2020-03-01|784246|\n",
      "| 2020-03-06|739715|\n",
      "| 2020-03-05|731165|\n",
      "| 2020-03-04|707879|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "select split(pickup_datetime, ' ')[0] as pickup_date, count(*) as trips\n",
    "from mobility_data\n",
    "\n",
    "group by pickup_date\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "30eb5a6f-2246-4e55-b6f1-aef9fdce364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['pickup_date], ['split('pickup_datetime,  )[0] AS pickup_date#3576, 'count(1) AS trips#3577]\n",
      "+- 'UnresolvedRelation [mobility_data], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "pickup_date: string, trips: bigint\n",
      "Aggregate [split(cast(pickup_datetime#3503 as string),  , -1)[0]], [split(cast(pickup_datetime#3503 as string),  , -1)[0] AS pickup_date#3576, count(1) AS trips#3577L]\n",
      "+- SubqueryAlias mobility_data\n",
      "   +- View (`mobility_data`, [hvfhs_license_num#3501,dispatching_base_num#3502,pickup_datetime#3503,dropoff_datetime#3504,PULocationID#3505,DOLocationID#3506,SR_Flag#3507])\n",
      "      +- Relation [hvfhs_license_num#3501,dispatching_base_num#3502,pickup_datetime#3503,dropoff_datetime#3504,PULocationID#3505,DOLocationID#3506,SR_Flag#3507] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [_groupingexpression#3581], [_groupingexpression#3581 AS pickup_date#3576, count(1) AS trips#3577L]\n",
      "+- Project [split(cast(pickup_datetime#3503 as string),  , -1)[0] AS _groupingexpression#3581]\n",
      "   +- Relation [hvfhs_license_num#3501,dispatching_base_num#3502,pickup_datetime#3503,dropoff_datetime#3504,PULocationID#3505,DOLocationID#3506,SR_Flag#3507] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[_groupingexpression#3581], functions=[count(1)], output=[pickup_date#3576, trips#3577L])\n",
      "   +- Exchange hashpartitioning(_groupingexpression#3581, 200), ENSURE_REQUIREMENTS, [plan_id=2823]\n",
      "      +- HashAggregate(keys=[_groupingexpression#3581], functions=[partial_count(1)], output=[_groupingexpression#3581, count#3583L])\n",
      "         +- Project [split(cast(pickup_datetime#3503 as string),  , -1)[0] AS _groupingexpression#3581]\n",
      "            +- FileScan csv [pickup_datetime#3503] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/learning_spark_data/fhvhv_tripdata_2020-03.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<pickup_datetime:timestamp>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실행 계획 살펴보기\n",
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de790c06-75d5-4f02-b4e0-213444ef1297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5fe4a0-a753-4fc1-b80a-67e49b31b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두번째 쿼리\n",
    "spark.sql(\"\"\"select \n",
    "                pickup_date, \n",
    "                count(*) as trips\n",
    "             from ( select\n",
    "                          split(pickup_datetime, ' ')[0] as pickup_date\n",
    "                          from mobility_data )\n",
    "             group by pickup_date\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754e8a1-6381-41a0-9d12-a6d7796b9500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d730f7-78ea-471f-946b-d0678ab227f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "17c80f14-19ff-4621-adaf-cbd4c0f34d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_file = \"fhvhv_tripdata_2020-03.csv\"\n",
    "zone_file = \"taxi+_zone_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f923da4c-899e-4cce-821f-258a455d0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"trip_count_sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "194499a8-a312-4e77-b884-0e1e4e31951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#운행 데이터 프레임 생성, Zone 데이터프레임 생성\n",
    "trip_data = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load('learning_spark_data/fhvhv_tripdata_2020-03.csv')\n",
    "zone_data = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load('learning_spark_data/taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9540d2b3-c34f-49b2-b024-7ba00880c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data.createOrReplaceTempView(\"trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5fda814d-9337-485e-b494-0a0aa5876890",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zone_data.createOrReplaceTempView(\"zone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "381485e6-7928-48b4-83aa-2caf2432d507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trip_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ff0ba2fb-9cb7-4282-9f8e-8943ea1ea09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "zone_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75066342-1808-42b9-80cd-87013dab06e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 승차 Location(PULocationID)별 개수 세기\n",
    "# 하차 Location(DOLocationID)별 개수 세기\n",
    "#HV0003 운송사업자의 승차 지역별 트립 건수를 집계하고, \n",
    "#가장 많은 운송사업자순으로 정렬하는 분석 쿼리  hvfhs_license_num\n",
    "#운송사별 운행 건수 비교\n",
    "#승차 위치 Borough별 운행 건수\n",
    "#서비스 존별 승차/하차 건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ec875dc2-b7c5-458a-b8b7-f75a298c0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2020-03-01 00:03:40|2020-03-01 00:23:39|          81|         159|   NULL|\n",
      "|           HV0005|              B02510|2020-03-01 00:28:05|2020-03-01 00:38:57|         168|         119|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:03:07|2020-03-01 00:15:04|         137|         209|      1|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4f4dc7ac-6c22-4c52-9c2d-43e3a222ffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------+\n",
      "|LocationID|Borough|                Zone|service_zone|\n",
      "+----------+-------+--------------------+------------+\n",
      "|         1|    EWR|      Newark Airport|         EWR|\n",
      "|         2| Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|  Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "+----------+-------+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "971e0e07-04ea-47b3-9e62-4c4fd4d1cda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|PULocationID|count(1)|\n",
      "+------------+--------+\n",
      "|         148|   41395|\n",
      "|         243|   25701|\n",
      "+------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # 승차 Location(PULocationID)별 개수 세기\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "select PULocationID, count(*)\n",
    "from trip\n",
    "group by PULocationID\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "255f8260-cbc3-4037-a047-8bce35656c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|DOLocationID|count(1)|\n",
      "+------------+--------+\n",
      "|         148|   31962|\n",
      "|         243|   25076|\n",
      "+------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 하차 Location(DOLocationID)별 개수 세기\n",
    "query = \"\"\"\n",
    "\n",
    "select DOLocationID, count(*)\n",
    "from trip\n",
    "group by DOLocationID\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b4b0fa69-7399-4334-9f4e-edd09af72ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|PULocationID|count(1)|\n",
      "+------------+--------+\n",
      "|         148|   26737|\n",
      "|         243|   20195|\n",
      "+------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#HV0003 운송사업자의 승차 지역별 트립 건수를 집계\n",
    "query = \"\"\"\n",
    "select PULocationID, count(*)\n",
    "from trip\n",
    "where hvfhs_license_num = 'HV0003'\n",
    "group by PULocationID\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "dfd92d7a-1206-4be8-ac76-39f1a05b7d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|hvfhs_license_num|count(1)|\n",
      "+-----------------+--------+\n",
      "|           HV0003| 3143107|\n",
      "|           HV0005| 1030267|\n",
      "|           HV0004|  136497|\n",
      "+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#가장 많은 운송사업자순으로 정렬하는 분석 쿼리  hvfhs_license_num\n",
    "query = \"\"\"\n",
    "select hvfhs_license_num, count(*)\n",
    "from trip\n",
    "group by hvfhs_license_num\n",
    "order by count(*) desc\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "723b73a0-514c-4aeb-8198-2a746b7c301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|hvfhs_license_num|count(1)|\n",
      "+-----------------+--------+\n",
      "|           HV0004|  136497|\n",
      "|           HV0005| 1030267|\n",
      "|           HV0003| 3143107|\n",
      "+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#운송사별 운행 건수 비교\n",
    "query = \"\"\"\n",
    "select hvfhs_license_num, count(*)\n",
    "from trip\n",
    "group by hvfhs_license_num\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a184d1a1-0c2f-4ac4-9e71-2d424732c1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|      Borough|count(1)|\n",
      "+-------------+--------+\n",
      "|       Queens|  763719|\n",
      "|          EWR|     132|\n",
      "|      Unknown|     231|\n",
      "|     Brooklyn| 1123416|\n",
      "|Staten Island|   55488|\n",
      "|    Manhattan| 1802744|\n",
      "|        Bronx|  564140|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#승차 위치 Borough별 운행 건수\n",
    "query = \"\"\"\n",
    "select Borough, count(*)\n",
    "from trip\n",
    "inner join zone\n",
    "on trip.PULocationID = zone.LocationID\n",
    "group by Borough\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3e7bee4b-25b1-4307-8900-a2751ead9227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2020-03-01 00:03:40|2020-03-01 00:23:39|          81|         159|   NULL|\n",
      "|           HV0005|              B02510|2020-03-01 00:28:05|2020-03-01 00:38:57|         168|         119|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:03:07|2020-03-01 00:15:04|         137|         209|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:18:42|2020-03-01 00:38:42|         209|          80|   NULL|\n",
      "|           HV0003|              B02764|2020-03-01 00:44:24|2020-03-01 00:58:44|         256|         226|   NULL|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9bca5c89-9e17-4633-bb4d-934f0e2056f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b214e6d4-2440-4b10-b439-0bfe0c0d4dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+\n",
      "|service_zone|count(PULocationID)|count(DOLocationID)|\n",
      "+------------+-------------------+-------------------+\n",
      "|         EWR|                132|                132|\n",
      "|         N/A|                231|                231|\n",
      "| Yellow Zone|            1535246|            1535246|\n",
      "|    Airports|             134404|             134404|\n",
      "|   Boro Zone|            2639857|            2639857|\n",
      "+------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#서비스 존별 승차/하차 건수\n",
    "\n",
    "query = \"\"\"\n",
    "select service_zone, count(PULocationID), count(DOLocationID)\n",
    "from trip\n",
    "inner join zone\n",
    "on trip.PULocationID = zone.LocationID\n",
    "group by service_zone\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211df98-012c-4e5a-8078-0ed288f9e857",
   "metadata": {},
   "source": [
    "# 택시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f219886f-f114-4641-b5e2-7c0bdb485db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"spark-sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d7b3d88-11c8-41e9-b0a6-5caf9b462d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format('json')\\\n",
    "    .load(\"learning_spark_data/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42867302-6944-4cea-9251-1749d87857bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEST_COUNTRY_NAME', 'string'),\n",
       " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
       " ('count', 'bigint')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd7b49f6-d084-4433-84f6-0e5d27f43db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b16eeb8-7ef8-43df-ae25-9e00ac192161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('count').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dacd511-8777-48a4-b33e-29f938dcd393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28f7a3a0-1511-40af-99a1-203a3e0c7a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 도착국가명 중복제거 \n",
    "#cache - 이 결과를 저장해서 다음부터 이걸 재사용해 \n",
    "df1 = df.select('DEST_COUNTRY_NAME').distinct().cache()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf582076-c610-432e-8445-d19279ff7c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row('hello', None, 1, False)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row class를 이용한 단일 레코드 생성 \n",
    "\n",
    "from pyspark.sql import Row\n",
    "myrow = Row('hello',None, 1, False)\n",
    "myrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d04a2dd-530b-400d-91d3-7e79472886bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, withinCountry: boolean]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 컬럼 추가하기\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df3 = df.withColumn('withinCountry',expr('ORIGIN_COUNTRY_NAME==DEST_COUNTRY_NAME')) #expr sql표현식을 받아 생성\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04b9e08b-cf70-49d6-8c82-8bd46a6e695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b69b62f5-a8b0-4923-a888-f32f4fd03837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|withinCountry|\n",
      "+-----------------+-------------------+------+-------------+\n",
      "|    United States|      United States|370002|         true|\n",
      "+-----------------+-------------------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.filter(df3[3] == 'True').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "581672e4-cbd0-49ff-ae32-6cd4e83a0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.filter(expr('withinCountry' == 'True')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6f107-dfa7-484a-9bff-a29b5c6c8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#case when 카운트 10 이하 under, 이상 upper로 변환 > category 컬럼 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48e8d6e4-686d-4291-862d-bee8bcc6bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+---------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|10_upperOrunder|\n",
      "+-----------------+-------------------+-----+-------------+---------------+\n",
      "|    United States|            Romania|   15|        false|          under|\n",
      "|    United States|            Croatia|    1|        false|          under|\n",
      "|    United States|            Ireland|  344|        false|          under|\n",
      "|            Egypt|      United States|   15|        false|          under|\n",
      "|    United States|              India|   62|        false|          under|\n",
      "+-----------------+-------------------+-----+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.withColumn('10_upperOrunder',expr(\"CASE WHEN 'count' >= 10 THEN 'upper' ELSE 'under'END\"))\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9458479-0410-4633-a433-7846b57e4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame의 select(), where(), filter() 트랜스포메이션\n",
    "# show(), count() 액션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "03d582d1-9654-4409-85e7-429d27b78fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e35b03-574a-4803-a1b0-c2bcb132b847",
   "metadata": {},
   "source": [
    "# empt, dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0c871e3a-fbba-4b76-b683-b1fefe8a11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"spark-sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7c3c3718-375f-4b1e-b7c8-d822f47a4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept=spark.read.format('csv')\\\n",
    "    .option('header','true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load(\"learning_spark_data/dept.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c8142c4-f16b-4943-b1ba-77f6630ec90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empt=spark.read.format('csv')\\\n",
    "    .option('header','true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load(\"learning_spark_data/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc63524c-f228-4dbe-a148-39ce55ccce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|deptno|     dname|     loc|\n",
      "+------+----------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|\n",
      "|    20|  RESEARCH|  DALLAS|\n",
      "|    30|     SALES| CHICAGO|\n",
      "|    40|OPERATIONS|  BOSTON|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dept.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd0401c9-87c4-4f42-a8d2-962f98c7ed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+\n",
      "|   _c0|  _c1|_c2|\n",
      "+------+-----+---+\n",
      "|deptno|dname|loc|\n",
      "+------+-----+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df1[column = df_dept.show(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34e15053-101c-4d08-b1a9-8b6fbefc8722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+----------+----+----+------+\n",
      "|empno| ename|     job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "| 7369| SMITH|   CLERK|7902|1980-12-17| 800|NULL|    20|\n",
      "| 7499| ALLEN|SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD|SALESMAN|7698|1981-02-22|1250| 500|    30|\n",
      "| 7566| JONES| MANAGER|7839|1981-04-02|2975|NULL|    20|\n",
      "| 7654|MARTIN|SALESMAN|7698|1981-09-28|1250|1400|    30|\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2eb881b6-8465-4c32-ba6f-67395ba33887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| ENAME|\n",
      "+------+\n",
      "| SMITH|\n",
      "| ALLEN|\n",
      "|  WARD|\n",
      "| JONES|\n",
      "|MARTIN|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#컬럼명을 대소문자 구분하지 않음 \n",
    "df_empt.select('ENAME').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25ac456d-a6ea-4cdd-a520-9d642d2d88b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+----+----------+----+----+------+\n",
      "|empno|ename|    job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+-----+-------+----+----------+----+----+------+\n",
      "| 7369|SMITH|  CLERK|7902|1980-12-17| 800|NULL|    20|\n",
      "| 7566|JONES|MANAGER|7839|1981-04-02|2975|NULL|    20|\n",
      "| 7788|SCOTT|ANALYST|7566|1987-04-19|3000|NULL|    20|\n",
      "| 7876|ADAMS|  CLERK|7788|1987-05-23|1100|NULL|    20|\n",
      "| 7902| FORD|ANALYST|7566|1981-12-03|3000|NULL|    20|\n",
      "+-----+-----+-------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter랑 동일함 \n",
    "df_empt.select('*').where('deptno=20').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5642be-7c78-42da-bcc1-6866deca0920",
   "metadata": {},
   "source": [
    "# 함수와 표현식(expr)의 차이점\n",
    "<details>\n",
    "<summary># 함수와 표현식(expr)의 차이점</summary>\n",
    "\n",
    "✅ 결론 먼저:\n",
    "\n",
    "```first('sal')```   \n",
    "는 함수(function) 사용이고,\n",
    "\n",
    "\n",
    "```expr(\"first(sal)\")```   \n",
    "는 표현식(expression) 사용이야.\n",
    "\n",
    "→ 둘은 결과는 비슷할 수 있지만 작동 방식은 다르다.   \n",
    "\n",
    "✅ 두 개의 차이점 비교   \n",
    "| 항목     | `first('sal')`       | `expr(\"first(sal)\")`  |\r\n",
    "| ------ | -------------------- | --------------------- |\r\n",
    "| 타입     | PySpark 함수 호출        | SQL 표현식 문자열           |\r\n",
    "| 추천 사용처 | DataFrame API        | SQL 함수, 동적 표현         |\r\n",
    "| 장점     | 안정적, 오타 적음           | 복잡한 조건, 문자열로 동적 생성 가능 |\r\n",
    "| 내부 동작  | Python에서 Spark 함수 실행 | SQL 파서가 전체 문자열 해석 \n",
    "\n",
    "✅ 예제로 비교    🔹 함수 기반   \n",
    "```from pyspark.sql.functions import first```   \n",
    "```\r\n",
    "df.select(first('sal')).show```    \n",
    "- first()는 PySpark의 내장 집계 함수\n",
    "- 직접 Python에서 함수처럼 호출함\n",
    "\n",
    "🔹 expr 기반   \n",
    "\n",
    "```from pyspark.sql.functions import expr```   \n",
    "```\r\n",
    "df.select(expr(\"first(sal)\")).show```   \n",
    "- SQL 표현식 전체를 문자열로 작성\n",
    "- when, case, math, date 함수들을 조합할 때 유리\n",
    "\n",
    "✅ 언제 expr이 유리할까?   \n",
    "- 복잡한 계산식을 문자열로 다룰 때   \n",
    "- 동적 SQL을 만들 때   \n",
    "- 여러 함수를 조합하거나 문자열 조건식을 써야 할 때\n",
    "\n",
    "✅ 요약   \n",
    "너가 쓴 first('sal')는 PySpark의 함수 사용이고   \n",
    "expr(\"first(sal)\")는 SQL 표현식을 문자열로 작성한 것   \n",
    "두 방식 다 가능하지만,   \n",
    "정확성 / 리팩토링엔 함수 방식   \n",
    "유연성 / 조건 표현엔 expr() 방식이 유리해.   ()()   |\r\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3be94202-af12-4b2d-9151-da759d780a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      15|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 조건식을 쓸거면 expr을 적어야함 \n",
    "\n",
    "df_empt.selectExpr('count(*)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7d4b9b7e-a2a2-47e6-ada8-bb21915a3cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT job)|\n",
      "+-------------------+\n",
      "|                  5|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df_empt.select(countDistinct('job')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "320b07a3-f1f0-4e6a-a0c4-cf7042094252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|approx_count_distinct(job)|\n",
      "+--------------------------+\n",
      "|                         5|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df_empt.select(approx_count_distinct('job',0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "26a747d2-a394-4421-88ff-53e4b5c1c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|first(sal)|\n",
      "+----------+\n",
      "|       800|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first (expr : sql문장 x), funtion으로 처리 \n",
    "from pyspark.sql.functions import first\n",
    "df_empt.select(first('sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28ba7170-1c01-4c2d-adbd-ee3455f67cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|last(sal)|\n",
      "+---------+\n",
      "|     3200|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# last\n",
    "from pyspark.sql.functions import last\n",
    "df_empt.select(last('sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4d4b1e4-e0b7-42ff-ae71-95d6e3b6ba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|min(sal)|\n",
      "+--------+\n",
      "|     800|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# min\n",
    "from pyspark.sql.functions import min\n",
    "df_empt.select(min('sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2426abda-5760-4950-808a-9f13b6f3cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(sal)|\n",
      "+--------+\n",
      "|    5000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max\n",
    "from pyspark.sql.functions import max\n",
    "df_empt.select(max('sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a4d53bf7-47b0-4652-83bd-2dd4117407cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sum(sal)|\n",
      "+--------+\n",
      "|   32225|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sum\n",
    "from pyspark.sql.functions import sum\n",
    "df_empt.select(sum('sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4df9c77a-e298-4d18-896c-1e7c3ee255e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|sum(sal)|min(sal)|\n",
      "+--------+--------+\n",
      "|   32225|     800|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions \n",
    "df_empt.select(sum('sal'),min('sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1f5d5058-2f8d-4f13-9466-012d3673702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+----+----------+----+----+------+\n",
      "|empno|ename|     job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+-----+--------+----+----------+----+----+------+\n",
      "| 7369|SMITH|   CLERK|7902|1980-12-17| 800|NULL|    20|\n",
      "| 7499|ALLEN|SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521| WARD|SALESMAN|7698|1981-02-22|1250| 500|    30|\n",
      "| 7566|JONES| MANAGER|7839|1981-04-02|2975|NULL|    20|\n",
      "+-----+-----+--------+----+----------+----+----+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empt.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cf5972a2-1839-45ef-9e75-ae0a978b5279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+------------------+\n",
      "|total_salary|total_transaction|        avg_salary|\n",
      "+------------+-----------------+------------------+\n",
      "|       32225|               15|2148.3333333333335|\n",
      "+------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#total_salary / total_transaction(샐러리의 카운트), avg_salary, mean_salary\n",
    "\n",
    "import pyspark.sql.functions \n",
    "df_empt1 = df_empt.selectExpr(\n",
    "    \"sum(sal) as total_salary\",\n",
    "    \"count(sal) as total_transaction\",\n",
    "    \"sum(sal)/count(sal) as avg_salary\"\n",
    ")\n",
    "\n",
    "df_empt1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0203f341-89ec-48fb-b577-39260ddb7231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------------------+------------------+\n",
      "|total_transaction|total_salary|        avg_salary|       mean_salary|\n",
      "+-----------------+------------+------------------+------------------+\n",
      "|               15|       32225|2148.3333333333335|2148.3333333333335|\n",
      "+-----------------+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import count,sum,avg,mean\n",
    "df_empt.select(\n",
    "    count(\"sal\").alias(\"total_transaction\"),\n",
    "    sum(\"sal\").alias(\"total_salary\"),\n",
    "    avg(\"sal\").alias(\"avg_salary\"),\n",
    "    mean(\"sal\").alias(\"mean_salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc1ddae-7a39-4043-8ada-ae251d0a2821",
   "metadata": {},
   "source": [
    "# 그룹화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e6011d35-3f01-4b42-849b-c567365750bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "|  ANALYST|    2|\n",
      "| SALESMAN|    4|\n",
      "|    CLERK|    5|\n",
      "|  MANAGER|    3|\n",
      "|PRESIDENT|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empt.groupBy('job').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b411bb6d-6927-4317-a48e-9190454dccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+--------+\n",
      "|      job|qty|count(job)|sum(sal)|\n",
      "+---------+---+----------+--------+\n",
      "|  ANALYST|  2|         2|    6000|\n",
      "| SALESMAN|  4|         4|    5600|\n",
      "|    CLERK|  5|         5|    7350|\n",
      "|  MANAGER|  3|         3|    8275|\n",
      "|PRESIDENT|  1|         1|    5000|\n",
      "+---------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select job,\n",
    "#     count(job)\n",
    "#     sum(sal)\n",
    "# groupBy job\n",
    "\n",
    "froup_df = df_empt.groupBy('job').agg(\n",
    "    count('job').alias('qty'),\n",
    "    expr('count(job)'),\n",
    "    sum('sal')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "052a5e08-1a2e-4c43-8fee-4aeb04f4c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+\n",
      "|      job|SAL_AVG|SAL_STDEV|\n",
      "+---------+-------+---------+\n",
      "|PRESIDENT| 5000.0|     NULL|\n",
      "|  ANALYST| 3000.0|      0.0|\n",
      "|  MANAGER|2758.33|   274.24|\n",
      "|    CLERK| 1470.0|   984.63|\n",
      "| SALESMAN| 1400.0|   177.95|\n",
      "+---------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sal의 평균 as SAL_AVG, 표준편차 as SAL_STDEV를 job별로 계산해서 출력 \n",
    "from pyspark.sql.functions import avg, stddev,round\n",
    "\n",
    "\n",
    "group1 = df_empt.groupby('job').agg(\n",
    "    round(avg('sal'),2).alias('SAL_AVG'),\n",
    "    round(stddev('sal'),2).alias('SAL_STDEV')\n",
    ")\n",
    "\n",
    "# 급여 평균 상위 10개 job 정렬 + 제한\n",
    "group1.orderBy('SAL_AVG', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b100b752-c5aa-4928-bc5a-7e3f5af903c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'RANK() OVER (ORDER BY sal DESC NULLS LAST unspecifiedframe$())'>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 윈도우함수\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, rank\n",
    "\n",
    "windowspec = Window.orderBy(desc('sal'))\n",
    "salAllRank = rank().over(windowspec)\n",
    "salAllRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bed2ae0d-842b-47c8-9932-9db72022d178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+----------+----+----+------+-----------+\n",
      "|empno|ename|      job| mgr|  hiredate| sal|comm|deptno|salary_rank|\n",
      "+-----+-----+---------+----+----------+----+----+------+-----------+\n",
      "| 7839| KING|PRESIDENT|NULL|1981-11-17|5000|NULL|    10|          1|\n",
      "| 9292| JACK|    CLERK|7782|1982-01-23|3200|NULL|    70|          2|\n",
      "+-----+-----+---------+----+----------+----+----+------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df_empt.withColumn('salary_rank', salAllRank)\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a1689a19-7f45-4dc6-85f4-865464cb81a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('empno', 'int'),\n",
       " ('ename', 'string'),\n",
       " ('job', 'string'),\n",
       " ('mgr', 'int'),\n",
       " ('hiredate', 'date'),\n",
       " ('sal', 'int'),\n",
       " ('comm', 'int'),\n",
       " ('deptno', 'int')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_empt.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f04ea58f-7b8f-4bd0-b616-e838a26047e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+-----------+\n",
      "| ename|    job| sal|job_rank_df|\n",
      "+------+-------+----+-----------+\n",
      "| SCOTT|ANALYST|3000|          1|\n",
      "|  FORD|ANALYST|3000|          1|\n",
      "|  JACK|  CLERK|3200|          1|\n",
      "|MILLER|  CLERK|1300|          2|\n",
      "| ADAMS|  CLERK|1100|          3|\n",
      "| JAMES|  CLERK| 950|          4|\n",
      "+------+-------+----+-----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 직무별로 rank 작성\n",
    "#window.partitionBy()\n",
    "#job_rank_df 작성\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, rank\n",
    "\n",
    "# 1. 윈도우 스펙: 직무별 그룹 + 급여 내림차순 정렬\n",
    "windowspec = Window.partitionBy('job').orderBy(desc('sal'))\n",
    "\n",
    "# 2. rank()를 사용하여 새 컬럼 생성\n",
    "job_rank_df = df_empt.withColumn(\"job_rank_df\", rank().over(windowspec))\n",
    "\n",
    "# 3. 결과 확인\n",
    "job_rank_df.select(\"ename\", \"job\", \"sal\",\"job_rank_df\").show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d63a3708-8f8b-4524-b8ff-20d3d9676fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+----+----------+----+----+------+\n",
      "|empno|ename|     job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+-----+--------+----+----------+----+----+------+\n",
      "| 7369|SMITH|   CLERK|7902|1980-12-17| 800|NULL|    20|\n",
      "| 7499|ALLEN|SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "+-----+-----+--------+----+----------+----+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empt.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "48f0b9e6-44df-4263-a404-1ce5ba103669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+----------+----+----+------+\n",
      "|empno| ename|     job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "| 7369| SMITH|   CLERK|7902|1980-12-17| 800|NULL|    20|\n",
      "| 7499| ALLEN|SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD|SALESMAN|7698|1981-02-22|1250| 500|    30|\n",
      "| 7566| JONES| MANAGER|7839|1981-04-02|2975|NULL|    20|\n",
      "| 7654|MARTIN|SALESMAN|7698|1981-09-28|1250|1400|    30|\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "016f9e6d-4478-4f37-9de2-464b4b6f5423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|      job|sum_sal|\n",
      "+---------+-------+\n",
      "|  MANAGER|   8275|\n",
      "|    CLERK|   7350|\n",
      "|  ANALYST|   6000|\n",
      "| SALESMAN|   5600|\n",
      "|PRESIDENT|   5000|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 부서별 순위\n",
    "\n",
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "df_rank = df_empt.groupBy('job').agg(\n",
    "    sum('sal').alias('sum_sal')\n",
    ")\n",
    "\n",
    "df_rank.orderBy('sum_sal', ascending=False).limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8030df33-6596-4d7a-8823-8ffd0be1758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+----+----------+----+----+------+-------+\n",
      "|empno| ename|    job| mgr|  hiredate| sal|comm|deptno|job_sum|\n",
      "+-----+------+-------+----+----------+----+----+------+-------+\n",
      "| 7788| SCOTT|ANALYST|7566|1987-04-19|3000|NULL|    20|   6000|\n",
      "| 7902|  FORD|ANALYST|7566|1981-12-03|3000|NULL|    20|   6000|\n",
      "| 7369| SMITH|  CLERK|7902|1980-12-17| 800|NULL|    20|   7350|\n",
      "| 7876| ADAMS|  CLERK|7788|1987-05-23|1100|NULL|    20|   7350|\n",
      "| 7900| JAMES|  CLERK|7698|1981-12-03| 950|NULL|    30|   7350|\n",
      "| 7934|MILLER|  CLERK|7782|1982-01-23|1300|NULL|    10|   7350|\n",
      "| 9292|  JACK|  CLERK|7782|1982-01-23|3200|NULL|    70|   7350|\n",
      "| 7566| JONES|MANAGER|7839|1981-04-02|2975|NULL|    20|   8275|\n",
      "| 7698| BLAKE|MANAGER|7839|1981-05-01|2850|NULL|    30|   8275|\n",
      "| 7782| CLARK|MANAGER|7839|1981-06-09|2450|NULL|    10|   8275|\n",
      "+-----+------+-------+----+----------+----+----+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 윈도우함수로 랭크 넣기\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "windowspec = Window.partitionBy('job').sum('sal')\n",
    "\n",
    "job_sum_df = df_empt.withColumn('job_sum', sum('sal').over(windowspec))\n",
    "\n",
    "job_sum_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6e38d425-4a01-4787-ab7c-75b9d2111203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|      job|sum(sal)|\n",
      "+---------+--------+\n",
      "|  ANALYST|    6000|\n",
      "| SALESMAN|    5600|\n",
      "|    CLERK|    7350|\n",
      "|  MANAGER|    8275|\n",
      "|PRESIDENT|    5000|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 부서별 누적급여\n",
    "\n",
    "df_empt.groupBy('job').agg(\n",
    "        sum('sal')\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4ae116fd-aa7e-4e5a-a402-3ad4e952b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+----+----------+----+----+------+------------------+\n",
      "|empno| ename|    job| mgr|  hiredate| sal|comm|deptno|           job_avg|\n",
      "+-----+------+-------+----+----------+----+----+------+------------------+\n",
      "| 7788| SCOTT|ANALYST|7566|1987-04-19|3000|NULL|    20|            3000.0|\n",
      "| 7902|  FORD|ANALYST|7566|1981-12-03|3000|NULL|    20|            3000.0|\n",
      "| 7369| SMITH|  CLERK|7902|1980-12-17| 800|NULL|    20|            1470.0|\n",
      "| 7876| ADAMS|  CLERK|7788|1987-05-23|1100|NULL|    20|            1470.0|\n",
      "| 7900| JAMES|  CLERK|7698|1981-12-03| 950|NULL|    30|            1470.0|\n",
      "| 7934|MILLER|  CLERK|7782|1982-01-23|1300|NULL|    10|            1470.0|\n",
      "| 9292|  JACK|  CLERK|7782|1982-01-23|3200|NULL|    70|            1470.0|\n",
      "| 7566| JONES|MANAGER|7839|1981-04-02|2975|NULL|    20|2758.3333333333335|\n",
      "| 7698| BLAKE|MANAGER|7839|1981-05-01|2850|NULL|    30|2758.3333333333335|\n",
      "| 7782| CLARK|MANAGER|7839|1981-06-09|2450|NULL|    10|2758.3333333333335|\n",
      "+-----+------+-------+----+----------+----+----+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 부서별 평균 급여와 직원 개별 급여 비교 \n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowspec= Window.partitionBy('job')\n",
    "\n",
    "job_avg_df = df_empt.withColumn('job_avg', avg('sal').over(windowspec))\n",
    "\n",
    "job_avg_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3fbd2-ed12-47dd-bc07-2326f553a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 윈도우 스펙: 직무별 그룹 + 급여 내림차순 정렬\n",
    "windowspec = Window.partitionBy('job').orderBy(desc('sal'))\n",
    "\n",
    "# 2. rank()를 사용하여 새 컬럼 생성\n",
    "job_rank_df = df_empt.withColumn(\"job_rank_df\", rank().over(windowspec))\n",
    "\n",
    "# 3. 결과 확인\n",
    "job_rank_df.select(\"ename\", \"job\", \"sal\",\"job_rank_df\").show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e743b1-6dc6-4b17-a4ee-b8d6b9a34d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2ad1c-8c7b-4ebc-b5e5-cc992baa5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 부서별 직업별 소계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "668232f0-b3a7-49bc-8cf8-487f502ba2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+--------+\n",
      "|deptno|      job|count(1)|sum(sal)|\n",
      "+------+---------+--------+--------+\n",
      "|    10|    CLERK|       1|    1300|\n",
      "|    10|  MANAGER|       1|    2450|\n",
      "|    10|PRESIDENT|       1|    5000|\n",
      "|    20|  ANALYST|       2|    6000|\n",
      "|    20|    CLERK|       2|    1900|\n",
      "|    20|  MANAGER|       1|    2975|\n",
      "|    30|    CLERK|       1|     950|\n",
      "|    30|  MANAGER|       1|    2850|\n",
      "|    30| SALESMAN|       4|    5600|\n",
      "|    70|    CLERK|       1|    3200|\n",
      "+------+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empt.groupBy('deptno', 'job').agg(count('*'), sum('sal'))\\\n",
    "        .orderBy('deptno', 'job').show() #평균급, 최대급, 최소급 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4c332442-cfcc-49ce-baea-3bb60ad849ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+--------+\n",
      "|deptno|      job|count(1)|sum(sal)|\n",
      "+------+---------+--------+--------+\n",
      "|  NULL|     NULL|      15|   32225|\n",
      "|  NULL|  ANALYST|       2|    6000|\n",
      "|  NULL|    CLERK|       5|    7350|\n",
      "|  NULL|  MANAGER|       3|    8275|\n",
      "|  NULL|PRESIDENT|       1|    5000|\n",
      "|  NULL| SALESMAN|       4|    5600|\n",
      "|    10|     NULL|       3|    8750|\n",
      "|    10|    CLERK|       1|    1300|\n",
      "|    10|  MANAGER|       1|    2450|\n",
      "|    10|PRESIDENT|       1|    5000|\n",
      "|    20|     NULL|       5|   10875|\n",
      "|    20|  ANALYST|       2|    6000|\n",
      "|    20|    CLERK|       2|    1900|\n",
      "|    20|  MANAGER|       1|    2975|\n",
      "|    30|     NULL|       6|    9400|\n",
      "|    30|    CLERK|       1|     950|\n",
      "|    30|  MANAGER|       1|    2850|\n",
      "|    30| SALESMAN|       4|    5600|\n",
      "|    70|     NULL|       1|    3200|\n",
      "|    70|    CLERK|       1|    3200|\n",
      "+------+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empt.cube('deptno', 'job').agg(count('*'), sum('sal'))\\\n",
    "    .orderBy('deptno', 'job').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa2735-e473-425a-82ba-e1a3caadbbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "838ade0c-6667-488b-ac55-42217570a633",
   "metadata": {},
   "source": [
    "# JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c9fb84c3-d740-4a1e-a264-644965136071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|NULL|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-22|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-04-02|2975|NULL|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-09-28|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|NULL|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|NULL|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1987-04-19|3000|NULL|    20|\n",
      "| 7839|  KING|PRESIDENT|NULL|1981-11-17|5000|NULL|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1981-09-08|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1987-05-23|1100|NULL|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|NULL|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-03|3000|NULL|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|NULL|    10|\n",
      "| 9292|  JACK|    CLERK|7782|1982-01-23|3200|NULL|    70|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "daeac0be-f669-4d8a-b22f-9dc3d40803ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|deptno|     dname|     loc|\n",
      "+------+----------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|\n",
      "|    20|  RESEARCH|  DALLAS|\n",
      "|    30|     SALES| CHICAGO|\n",
      "|    40|OPERATIONS|  BOSTON|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b5dfcea1-7fc0-403e-a403-c51a5abb5523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|deptno|     dname|     loc|\n",
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|NULL|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|    30|     SALES| CHICAGO|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-22|1250| 500|    30|    30|     SALES| CHICAGO|\n",
      "| 7566| JONES|  MANAGER|7839|1981-04-02|2975|NULL|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-09-28|1250|1400|    30|    30|     SALES| CHICAGO|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|NULL|    30|    30|     SALES| CHICAGO|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|NULL|    10|    10|ACCOUNTING|NEW YORK|\n",
      "| 7788| SCOTT|  ANALYST|7566|1987-04-19|3000|NULL|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7839|  KING|PRESIDENT|NULL|1981-11-17|5000|NULL|    10|    10|ACCOUNTING|NEW YORK|\n",
      "| 7844|TURNER| SALESMAN|7698|1981-09-08|1500|   0|    30|    30|     SALES| CHICAGO|\n",
      "| 7876| ADAMS|    CLERK|7788|1987-05-23|1100|NULL|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|NULL|    30|    30|     SALES| CHICAGO|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-03|3000|NULL|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|NULL|    10|    10|ACCOUNTING|NEW YORK|\n",
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept_df = df_empt.join(df_dept, df_empt['deptno'] == df_dept['deptno'])\n",
    "emp_dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cb02763a-f411-49b9-a661-5d4a23d78b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n",
      "| ename|deptno|     dname|\n",
      "+------+------+----------+\n",
      "| SMITH|    20|  RESEARCH|\n",
      "| ALLEN|    30|     SALES|\n",
      "|  WARD|    30|     SALES|\n",
      "| JONES|    20|  RESEARCH|\n",
      "|MARTIN|    30|     SALES|\n",
      "| BLAKE|    30|     SALES|\n",
      "| CLARK|    10|ACCOUNTING|\n",
      "| SCOTT|    20|  RESEARCH|\n",
      "|  KING|    10|ACCOUNTING|\n",
      "|TURNER|    30|     SALES|\n",
      "| ADAMS|    20|  RESEARCH|\n",
      "| JAMES|    30|     SALES|\n",
      "|  FORD|    20|  RESEARCH|\n",
      "|MILLER|    10|ACCOUNTING|\n",
      "+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = df_empt.join(df_dept, on='deptno', how='inner')\n",
    "join_df.select('ename', 'deptno', 'dname').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a853d-5087-4499-b175-8a97b59af259",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
